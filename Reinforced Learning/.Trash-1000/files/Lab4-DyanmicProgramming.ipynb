{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dynamic Programming Lab **  \n",
    "\n",
    "This is the Lab for the Dynamic Programming module of the edX \"Reinforcement Learning Explained\" course.  The lab consists of 4 exercises:\n",
    "\n",
    "   - implement Policy Evaluation using the 2 array approach\n",
    "   - implement Policy Evaluation using the in-place approach\n",
    "   - implement Policy Iteration \n",
    "   - implement Value Iteration\n",
    "   \n",
    "On each of the 4 code cells below (one for each exercise), make sure you don't change the function signature for the primary function you are implementing, and the call to the tester code that verifies its correctness.\n",
    "\n",
    "When you finish your implemention of each function, execute the code cell and vertify that the code passes.  If it does, save the printed \"passcode\" value for when you later submit your results on the course webpage for the lab.  If it doesn't pass, correct your code and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1: Policy Evaluation - 2 arrays **  \n",
    "\n",
    "Policy Evaluation calculates the value function for a policy, given the policy and the full definition of the associated Markov Decision Process.  The full definition of an MDP is the set of states, the set of available actions for each state, the set of rewards, the discount factor, and the state/reward transition function.\n",
    "\n",
    "Implement the algorithm for Iterative Policy Evaluation using the 2 array approach in the below code cell.  In the 2 array approach, one array holds the value estimates for each state computed on the previous iteration, and one array holds the value estimates for the states computing in the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (two-arrays)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "ERROR: list elements don't match expected values: # of mismatches=14\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def policy_eval_two_arrays(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function uses the two-array approach to evaluate the specified policy for the specified MDP:\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_policy_actions' is the stochastic policy function - it takes a state parameter and returns list of tuples, \n",
    "        where each tuple is of the form: (action, probability).  It represents the policy being evaluated.\n",
    "    'get_transitions' is the state/reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]\n",
    "    # insert code here to evaluate the policy using the 2 array approach \n",
    "    return V\n",
    "\n",
    "tester.policy_eval_two_arrays_test(policy_eval_two_arrays)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (two-arrays)\n",
      "FINAL k= 173\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (two-arrays) passcode = 9986-145\n"
     ]
    }
   ],
   "source": [
    "#--- Solutions below - remove all below code cells on the student version of the labs ---\n",
    "import tester\n",
    "\n",
    "def eval_formula2(state, action, get_transitions, gamma, V):\n",
    "    trans_sum = 0\n",
    "    trans_tuples = get_transitions(state, action)\n",
    "    for tt in trans_tuples:\n",
    "        next_state = tt[0]\n",
    "        reward = tt[1]\n",
    "        trans_prob = tt[2]\n",
    "        trans_sum += trans_prob * (reward + gamma * V[next_state])\n",
    "    return trans_sum\n",
    "\n",
    "def eval_formula(state, state_action_tuples, get_transitions, gamma, V):\n",
    "    action_sum = 0\n",
    "    for at in state_action_tuples:\n",
    "        action = at[0]\n",
    "        action_prob = at[1]\n",
    "        action_sum += action_prob * eval_formula2(state, action, get_transitions, gamma, V)\n",
    "    return action_sum\n",
    "\n",
    "def policy_eval_two_arrays(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    V = state_count*[0]\n",
    "    V_last = state_count*[0]\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        #print(\"k=\", k, \"V=\", V)\n",
    "\n",
    "        for s in range(state_count):\n",
    "            v = V_last[s]\n",
    "            state_action_tuples = get_policy_actions(s)\n",
    "            \n",
    "            V[s] = eval_formula(s, state_action_tuples, get_transitions, gamma, V_last)\n",
    "\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        k += 1\n",
    "        if (delta < theta):\n",
    "            break\n",
    "        V_last = list(V)\n",
    "\n",
    "    print(\"FINAL k=\", k)\n",
    "    #print(\"FINAL V=\", V)\n",
    "    return V\n",
    "\n",
    "tester.policy_eval_two_arrays_test(policy_eval_two_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 0.25), ('u', 0.25), ('r', 0.25), ('d', 0.25)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.get_equiprobable_policy_actions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -1, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.get_transitions(2,'l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l 0.25\n",
      "u 0.25\n",
      "r 0.25\n",
      "d 0.25\n"
     ]
    }
   ],
   "source": [
    "for a, action in testSuper.get_equiprobable_policy_actions(0):\n",
    "    print(a, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (two-arrays)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (two-arrays) passcode = 9986-145\n"
     ]
    }
   ],
   "source": [
    "def policy_eval_two_arrays(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    #V = np.zeros(env.nS)\n",
    "    V = state_count*[0]\n",
    "    V_last = state_count*[0]\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(state_count):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in get_policy_actions(s):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  next_state, reward, prob in get_transitions(s,a):\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + gamma * V_last[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "        V_last = list(V)\n",
    "    return V\n",
    "tester.policy_eval_two_arrays_test(policy_eval_two_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 2: Policy Evaluation - in-place method **  \n",
    "\n",
    "Implement the algorithm for Iterative Policy Evaluation using the in-place approach in the below code cell.  In the in-place approach, one array holds the values being estimated for each state and the same array is used for estimates of states needed by the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "ERROR: list elements don't match expected values: # of mismatches=14\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def policy_eval_in_place(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified policy for the specified MDP:\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_policy_actions' is the stochastic policy function - it takes a state parameter and returns list of tuples, \n",
    "        where each tuple is of the form: (action, probability).  It represents the policy being evaluated.\n",
    "    'get_transitions' is the state/reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]\n",
    "    # insert code here to evaluate the policy using the in-place approach \n",
    "    return V\n",
    "\n",
    "tester.policy_eval_in_place_test(policy_eval_in_place)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "FINAL k= 114\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (in-place) passcode = 9991-562\n"
     ]
    }
   ],
   "source": [
    "#--- Solutions below - remove all below code cells on the student version of the labs ---\n",
    "import tester\n",
    "\n",
    "def eval_formula2(state, action, get_transitions, gamma, V):\n",
    "    trans_sum = 0\n",
    "    trans_tuples = get_transitions(state, action)\n",
    "    for tt in trans_tuples:\n",
    "        next_state = tt[0]\n",
    "        reward = tt[1]\n",
    "        trans_prob = tt[2]\n",
    "        trans_sum += trans_prob * (reward + gamma * V[next_state])\n",
    "    return trans_sum\n",
    "\n",
    "def eval_formula(state, state_action_tuples, get_transitions, gamma, V):\n",
    "    action_sum = 0\n",
    "    for at in state_action_tuples:\n",
    "        action = at[0]\n",
    "        action_prob = at[1]\n",
    "        action_sum += action_prob * eval_formula2(state, action, get_transitions, gamma, V)\n",
    "    return action_sum\n",
    "\n",
    "def policy_eval_in_place(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    V = state_count*[0]\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        #print(\"k=\", k, \"V=\", V)\n",
    "\n",
    "        for s in range(state_count):\n",
    "            v = V[s]\n",
    "            state_action_tuples = get_policy_actions(s)\n",
    "            \n",
    "            V[s] = eval_formula(s, state_action_tuples, get_transitions, gamma, V)\n",
    "\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        k += 1\n",
    "        if (delta < theta):\n",
    "            break\n",
    "\n",
    "    print(\"FINAL k=\", k)\n",
    "    #print(\"FINAL V=\", V)\n",
    "    return V\n",
    "\n",
    "tester.policy_eval_in_place_test(policy_eval_in_place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (in-place) passcode = 9991-562\n"
     ]
    }
   ],
   "source": [
    "def policy_eval_in_place(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    #V = np.zeros(env.nS)\n",
    "    V = state_count*[0]\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(state_count):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in get_policy_actions(s):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  next_state, reward, prob in get_transitions(s,a):\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "tester.policy_eval_in_place_test(policy_eval_in_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 3: Policy Iteration **  \n",
    "\n",
    "Implement the algorithm for Policy Iteration in the code cell below.  \n",
    "\n",
    "** Can I just call \"policy_eval_in_place()\" for the Policy Evaluation step of this algorithm? **  \n",
    "\n",
    "Note that there is a subtle difference between the algorithm for Policy Evaluation, which assumes the policy is stochastic, and the Policy Evaluation step for the Policy Iteration algorithm, which assumes the policy is deterministic.  This means that you cannot directly call your previous code, but you can reuse large pieces of it for the Policy Evaluation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "ERROR: v elements don't match expected values: # of mismatches=14\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def policy_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Policy Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]                # init all state value estimates to 0\n",
    "    pi = state_count*[0]\n",
    "    \n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "        \n",
    "    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.policy_iteration_test(policy_iteration)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 4: Value Iteration **  \n",
    "\n",
    "Implement the algorithm for Value Iteration in the code cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Value Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "ERROR: v elements don't match expected values: # of mismatches=14\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def value_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Value Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]                # init all state value estimates to 0\n",
    "    pi = state_count*[0]\n",
    "    \n",
    "    # (this section of code can be removed when actual implementation is added)\n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "        \n",
    "    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.value_iteration_test(value_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (two-arrays)\n",
      "FINAL k= 173\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (two-arrays) passcode = 9986-145\n",
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "FINAL k= 114\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (in-place) passcode = 9991-562\n"
     ]
    }
   ],
   "source": [
    "#--- Solutions below - remove all below code cells on the student version of the labs ---\n",
    "import tester\n",
    "\n",
    "def eval_formula2(state, action, get_transitions, gamma, V):\n",
    "    trans_sum = 0\n",
    "    trans_tuples = get_transitions(state, action)\n",
    "    for tt in trans_tuples:\n",
    "        next_state = tt[0]\n",
    "        reward = tt[1]\n",
    "        trans_prob = tt[2]\n",
    "        trans_sum += trans_prob * (reward + gamma * V[next_state])\n",
    "    return trans_sum\n",
    "\n",
    "def eval_formula(state, state_action_tuples, get_transitions, gamma, V):\n",
    "    action_sum = 0\n",
    "    for at in state_action_tuples:\n",
    "        action = at[0]\n",
    "        action_prob = at[1]\n",
    "        action_sum += action_prob * eval_formula2(state, action, get_transitions, gamma, V)\n",
    "    return action_sum\n",
    "\n",
    "def policy_eval_in_place(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    V = state_count*[0]\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        #print(\"k=\", k, \"V=\", V)\n",
    "\n",
    "        for s in range(state_count):\n",
    "            v = V[s]\n",
    "            state_action_tuples = get_policy_actions(s)\n",
    "            \n",
    "            V[s] = eval_formula(s, state_action_tuples, get_transitions, gamma, V)\n",
    "\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        k += 1\n",
    "        if (delta < theta):\n",
    "            break\n",
    "\n",
    "    print(\"FINAL k=\", k)\n",
    "    #print(\"FINAL V=\", V)\n",
    "    return V\n",
    "\n",
    "def policy_eval_two_arrays(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    V = state_count*[0]\n",
    "    V_last = state_count*[0]\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        #print(\"k=\", k, \"V=\", V)\n",
    "\n",
    "        for s in range(state_count):\n",
    "            v = V_last[s]\n",
    "            state_action_tuples = get_policy_actions(s)\n",
    "            \n",
    "            V[s] = eval_formula(s, state_action_tuples, get_transitions, gamma, V_last)\n",
    "\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        k += 1\n",
    "        if (delta < theta):\n",
    "            break\n",
    "        V_last = list(V)\n",
    "\n",
    "    print(\"FINAL k=\", k)\n",
    "    #print(\"FINAL V=\", V)\n",
    "    return V\n",
    "\n",
    "tester.policy_eval_two_arrays_test(policy_eval_two_arrays)\n",
    "tester.policy_eval_in_place_test(policy_eval_in_place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Iteration\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "final V= [0.0, -1.0, -1.999, -2.997001, -1.0, -1.999, -2.997001, -1.999, -1.999, -2.997001, -1.999, -1.0, -2.997001, -1.999, -1.0]\n",
      "final pi= ['d', 'l', 'l', 'd', 'u', 'u', 'd', 'd', 'u', 'd', 'd', 'd', 'r', 'r', 'r']\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "passed test: values of v elements\n",
      "passed test: pi is list of length=15\n",
      "passed test: values of pi elements\n",
      "PASSED: Policy Iteration passcode = 9970-010\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def calc_max_action(state, avail_actions, get_transitions, gamma, V):\n",
    "    max_action = avail_actions[0]\n",
    "    max_value = -999999\n",
    "    \n",
    "    for action in avail_actions:\n",
    "        value = eval_formula3(state, action, get_transitions, gamma, V)\n",
    "        if (value >= max_value):\n",
    "            max_value = value\n",
    "            max_action = action\n",
    "            \n",
    "    #print(\"avail_actions=\", avail_actions, \", max_action=\", max_action, \", max_value=\", max_value)\n",
    "    return max_action\n",
    "\n",
    "def eval_formula3(state, action, get_transitions, gamma, V):\n",
    "    trans_sum = 0\n",
    "    trans_tuples = get_transitions(state, action)\n",
    "    for tt in trans_tuples:\n",
    "        next_state = tt[0]\n",
    "        reward = tt[1]\n",
    "        trans_prob = tt[2]\n",
    "        trans_sum += trans_prob * (reward + gamma * V[next_state])\n",
    "    return trans_sum\n",
    "\n",
    "def deterministic_policy_eval(state_count, gamma, theta, pi, get_transitions):\n",
    "    V = state_count*[0]\n",
    "    k = 0\n",
    "    #print(\"deterministic_policy_eval: theta=\", theta)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        #print(\"k=\", k, \"V=\", V)\n",
    "\n",
    "        for s in range(state_count):\n",
    "            v = V[s]\n",
    "            at = pi[s]\n",
    "            action = at[0]\n",
    "            V[s] = eval_formula3(s, action, get_transitions, gamma, V)\n",
    "            delta = max(delta, abs(v-V[s]))\n",
    "        k += 1\n",
    "        #print(\"k=\", k, \"delta=\", delta)\n",
    "        if (delta < theta):\n",
    "            break\n",
    "\n",
    "    #print(\"  Policy Eval step completed: k=\", k)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    # step 1 - initialization\n",
    "    V = state_count * [0]                # init all state value estimates to 0\n",
    "    pi = state_count * [0]           \n",
    "    \n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "    \n",
    "    iteration = 1\n",
    "    while (True):\n",
    "        print(\"Iteration: \" + str(iteration))\n",
    "        \n",
    "        # step 2 - Policy Evaluation\n",
    "        V = deterministic_policy_eval(state_count, gamma, theta, pi, get_transitions)\n",
    "\n",
    "        # step 3 - Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(state_count):\n",
    "            old_action = pi[s]\n",
    "            avail_actions = get_available_actions(s)\n",
    "            pi[s] = calc_max_action(s, avail_actions, get_transitions, gamma, V)\n",
    "            if (old_action != pi[s]):\n",
    "                policy_stable = False\n",
    "        #print(\"  Policy Improvement step completed\")\n",
    "\n",
    "        if policy_stable:\n",
    "            V = deterministic_policy_eval(state_count, gamma, theta, pi, get_transitions)\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    print(\"final V=\", V)\n",
    "    print(\"final pi=\", pi)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.policy_iteration_test(policy_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Value Iteration\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "final V= [0.0, -1.0, -1.999, -2.997001, -1.0, -1.999, -2.997001, -1.999, -1.999, -2.997001, -1.999, -1.0, -2.997001, -1.999, -1.0]\n",
      "final pi= ['d', 'l', 'l', 'd', 'u', 'u', 'd', 'd', 'u', 'd', 'd', 'd', 'r', 'r', 'r']\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "passed test: values of v elements\n",
      "passed test: pi is list of length=15\n",
      "passed test: values of pi elements\n",
      "PASSED: Value Iteration passcode = 9990-000\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "\n",
    "def eval_formula3(state, action, get_transitions, gamma, V):\n",
    "    trans_sum = 0\n",
    "    trans_tuples = get_transitions(state, action)\n",
    "    for tt in trans_tuples:\n",
    "        next_state = tt[0]\n",
    "        reward = tt[1]\n",
    "        trans_prob = tt[2]\n",
    "        trans_sum += trans_prob * (reward + gamma * V[next_state])\n",
    "    return trans_sum\n",
    "\n",
    "def calc_max_action_value(state, avail_actions, get_transitions, gamma, V):\n",
    "    max_action = avail_actions[0]\n",
    "    max_value = -999999\n",
    "    \n",
    "    for action in avail_actions:\n",
    "        value = eval_formula3(state, action, get_transitions, gamma, V)\n",
    "        if (value >= max_value):\n",
    "            max_value = value\n",
    "            max_action = action\n",
    "            \n",
    "    #print(\"avail_actions=\", avail_actions, \", max_action=\", max_action, \", max_value=\", max_value)\n",
    "    return (max_action, max_value)\n",
    "\n",
    "def value_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    V = state_count * [0]                # init all state value estimates to 0\n",
    "    iteration = 1\n",
    "\n",
    "    while (True):\n",
    "        print(\"Iteration: \" + str(iteration))\n",
    "        delta = 0\n",
    "        for s in range(state_count):\n",
    "            v = V[s]\n",
    "            avail_actions = get_available_actions(s)\n",
    "            _, V[s] = calc_max_action_value(s, avail_actions, get_transitions, gamma, V)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if (delta < theta):\n",
    "            break\n",
    "        iteration += 1\n",
    "\n",
    "    # finally, calculate the optimal policy from the optimal value function V\n",
    "    pi = state_count * [0]           \n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s], _ = calc_max_action_value(s, avail_actions, get_transitions, gamma, V)\n",
    "\n",
    "    print(\"final V=\", V)\n",
    "    print(\"final pi=\", pi)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.value_iteration_test(value_iteration)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
